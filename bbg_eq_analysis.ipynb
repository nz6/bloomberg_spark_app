{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ADR_ADR_PER_SH=' '),\n",
       " Row(ADR_ADR_PER_SH='.000083'),\n",
       " Row(ADR_ADR_PER_SH='.000100'),\n",
       " Row(ADR_ADR_PER_SH='.000500'),\n",
       " Row(ADR_ADR_PER_SH='.000667'),\n",
       " Row(ADR_ADR_PER_SH='.001000'),\n",
       " Row(ADR_ADR_PER_SH='.001111'),\n",
       " Row(ADR_ADR_PER_SH='.001667'),\n",
       " Row(ADR_ADR_PER_SH='.002000'),\n",
       " Row(ADR_ADR_PER_SH='.002500'),\n",
       " Row(ADR_ADR_PER_SH='.005000'),\n",
       " Row(ADR_ADR_PER_SH='.010000'),\n",
       " Row(ADR_ADR_PER_SH='.013333'),\n",
       " Row(ADR_ADR_PER_SH='.016667'),\n",
       " Row(ADR_ADR_PER_SH='.020000'),\n",
       " Row(ADR_ADR_PER_SH='.022222'),\n",
       " Row(ADR_ADR_PER_SH='.025000'),\n",
       " Row(ADR_ADR_PER_SH='.028571'),\n",
       " Row(ADR_ADR_PER_SH='.033333'),\n",
       " Row(ADR_ADR_PER_SH='.040000'),\n",
       " Row(ADR_ADR_PER_SH='.050000'),\n",
       " Row(ADR_ADR_PER_SH='.055556'),\n",
       " Row(ADR_ADR_PER_SH='.062500'),\n",
       " Row(ADR_ADR_PER_SH='.066667'),\n",
       " Row(ADR_ADR_PER_SH='.076923'),\n",
       " Row(ADR_ADR_PER_SH='.083333'),\n",
       " Row(ADR_ADR_PER_SH='.100000'),\n",
       " Row(ADR_ADR_PER_SH='.125000'),\n",
       " Row(ADR_ADR_PER_SH='.142857'),\n",
       " Row(ADR_ADR_PER_SH='.166667'),\n",
       " Row(ADR_ADR_PER_SH='.200000'),\n",
       " Row(ADR_ADR_PER_SH='.250000'),\n",
       " Row(ADR_ADR_PER_SH='.333333'),\n",
       " Row(ADR_ADR_PER_SH='.400000'),\n",
       " Row(ADR_ADR_PER_SH='.500000'),\n",
       " Row(ADR_ADR_PER_SH='.666667'),\n",
       " Row(ADR_ADR_PER_SH='1.000000'),\n",
       " Row(ADR_ADR_PER_SH='1.500015'),\n",
       " Row(ADR_ADR_PER_SH='10.000000'),\n",
       " Row(ADR_ADR_PER_SH='100.000000'),\n",
       " Row(ADR_ADR_PER_SH='139.999200'),\n",
       " Row(ADR_ADR_PER_SH='2.000000'),\n",
       " Row(ADR_ADR_PER_SH='20.000000'),\n",
       " Row(ADR_ADR_PER_SH='200.000000'),\n",
       " Row(ADR_ADR_PER_SH='2000.000000'),\n",
       " Row(ADR_ADR_PER_SH='3.000000'),\n",
       " Row(ADR_ADR_PER_SH='3.000030'),\n",
       " Row(ADR_ADR_PER_SH='3.000300'),\n",
       " Row(ADR_ADR_PER_SH='3.030303'),\n",
       " Row(ADR_ADR_PER_SH='3.333333'),\n",
       " Row(ADR_ADR_PER_SH='4.000000'),\n",
       " Row(ADR_ADR_PER_SH='5.000000'),\n",
       " Row(ADR_ADR_PER_SH='5.999880'),\n",
       " Row(ADR_ADR_PER_SH='50.000000'),\n",
       " Row(ADR_ADR_PER_SH='6.000000'),\n",
       " Row(ADR_ADR_PER_SH='6.000024'),\n",
       " Row(ADR_ADR_PER_SH='6.000240'),\n",
       " Row(ADR_ADR_PER_SH='8.000000'),\n",
       " Row(ADR_ADR_PER_SH='9.000000'),\n",
       " Row(ADR_ADR_PER_SH='N.A.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('bbg_credit_risk').setMaster('local')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "datafile = sc.textFile('D:\\\\NeoXam DataHub\\\\Data\\\\import\\\\bloomberg\\\\datafiles\\\\bulk\\\\BBG Back Office Samples\\\\EQUITY\\\\equity_euro.out.20160714')\n",
    "\n",
    "headerlist = datafile.take(500)\n",
    "headerrdd = sc.parallelize(headerlist)\n",
    "header = headerrdd.map(lambda line: line.split(\"|\"))\\\n",
    "    .filter(lambda line: line[0].find('START-OF')<0 and line[0].find('END-OF')<0)\\\n",
    "    .filter(lambda line: len(line)==1 and len(line[0])>0 and line[0].find('=')<0 and line[0].find('#')<0)\\\n",
    "    .map(lambda line: line[0])\\\n",
    "    .collect()\n",
    "\n",
    "header = ['ID', 'RETURN_CODE', 'NUMBER_OF_FIELDS'] + header + ['LAST_EMPTY_COLUMN']\n",
    "fields = [StructField(field_name, StringType(), True)\n",
    "      for field_name in header]\n",
    "\n",
    "schema = StructType(fields)\n",
    "data = datafile.map(lambda line: line.split(\"|\")).filter(lambda line: len(line)>1)\n",
    "data_df = sqlContext.createDataFrame(data, schema)\n",
    "data_df.createOrReplaceTempView(\"EQUITY\")\n",
    "#data_df2 = spark.sql(\"select max(case PAR_AMT when 'N.A.' then -1 else PAR_AMT end) as max_par_amt, min(PAR_AMT) as min_par_amt from EQUITY\")\n",
    "#data_df2 = spark.sql(\"select max(case TOTAL_NON_VOTING_SHARES_VALUE when 'N.A.' then -1 else TOTAL_NON_VOTING_SHARES_VALUE end) as max_par_amt, min(TOTAL_NON_VOTING_SHARES_VALUE) as min_par_amt from EQUITY\")\n",
    "#data_df2 = spark.sql(\"select count(*) from EQUITY group by DVD_FREQ\")\n",
    "#data_df2 = spark.sql(\"select DVD_FREQ, count(*) as count_dvd_freq_nb, count(*)/sum(count(*)) over() as pct from EQUITY group by DVD_FREQ\")\n",
    "\n",
    "data_df2 = spark.sql(\"select distinct(ADR_ADR_PER_SH) from EQUITY order by ADR_ADR_PER_SH\")\n",
    "res = data_df2.collect()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
